{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a57b6e53",
   "metadata": {},
   "source": [
    "# Lab 02: Explore Built-in Quality Evaluators\n",
    "\n",
    "By the end of this lab, you will know:\n",
    "\n",
    "1. What AI-Assisted evaluation workflows are, and how to run them.\n",
    "1. The built-in quality evaluators available in Azure AI Foundry\n",
    "1. How to run a quality evaluator with a test prompt (to understand usage)\n",
    "1. How to run a composite quality evaluator (with multiple evaluators)\n",
    "\n",
    "**Generation Quality Metrics**\n",
    "\n",
    "1. These are used to assess the overall quality of the content produced by generative AI applications. \n",
    "1. All metrics or evaluators output a score and an explanation (except for SimilarityEvaluator which has score only). \n",
    "1. [Browse the documentation](https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/evaluation-metrics-built-in?tabs=warning#generation-quality-metrics) for details on how each metric works.\n",
    "\n",
    "**Built-in Generation Quality Evaluators**\n",
    "\n",
    "The Azure AI Foundry plaform provides a comprehensive set of built-in quality evaluators that can be used to assess the performance of generative AI models. \n",
    "- Visit the [documentation](https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/evaluation-metrics-built-in?tabs=warning#generation-quality-metrics) to get the latest updates\n",
    "- Visit the [API reference](https://learn.microsoft.com/en-us/python/api/azure-ai-evaluation/azure.ai.evaluation?view=azure-python-preview) to understand usage of the API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5c2dc7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Initialize Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa611ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'project_name': 'ai-project-51324400',\n",
      " 'resource_group_name': 'rg-aitour',\n",
      " 'subscription_id': '3c2e0a23-bcf8-4766-84b7-8c635df04a7b'}\n",
      "{'api_key': '55b32d2e39584a7f9a17fa750261ffb7',\n",
      " 'azure_deployment': 'gpt-4',\n",
      " 'azure_endpoint': 'https://aoai-51324400.openai.azure.com/'}\n",
      "<azure.identity._credentials.default.DefaultAzureCredential object at 0x7ccc7f1c85f0>\n"
     ]
    }
   ],
   "source": [
    "## Setup Required Dependencies\n",
    "\n",
    "# --------- Azure AI Project\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "# The Azure AI Foundry connection string contains all the parameters we need\n",
    "connection_string = os.environ.get(\"AZURE_AI_CONNECTION_STRING\")\n",
    "region_id, subscription_id, resource_group_name, project_name = connection_string.split(\";\")\n",
    "\n",
    "# Use extracted values to create the azure_ai_project\n",
    "azure_ai_project = {\n",
    "    \"subscription_id\": subscription_id,\n",
    "    \"resource_group_name\": resource_group_name,\n",
    "    \"project_name\": project_name,\n",
    "}\n",
    "pprint(azure_ai_project)\n",
    "\n",
    "# ---------- Model Config\n",
    "model_config = {\n",
    "    \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    \"api_key\": os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    \"azure_deployment\": os.environ.get(\"LAB_JUDGE_MODEL\"),\n",
    "}\n",
    "pprint(model_config)\n",
    "\n",
    "# ---------- Azure Credential\n",
    "from azure.identity import DefaultAzureCredential\n",
    "credential=DefaultAzureCredential()\n",
    "pprint(credential)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04a518b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. General Purpose Evaluators    \n",
    "\n",
    "These are evaluators that look at the quality of textual responses in generated cotent and include:\n",
    "1. Coherence - measures the logical and orderly presentation of ideas in a response, allowing the reader to easily follow and understand the writer's train of thought\n",
    "1. Fluency - measures the effectiveness and clarity of written communication, focusing on grammatical accuracy, vocabulary range, sentence complexity, coherence, and overall readability\n",
    "1. QA Composite - measures comprehensively various aspects in a question-answering scenario including relevance, groundedness, fluency, coherence, similarity, and F1 score.\n",
    "\n",
    "Scores are typically numerical, generated using a Likert scale (1 to 5) with higher scores indicating better quality. The _threshold_ sets the cutoff for a \"pass/fail\" rating on that evaluator, helping you get a quick sense of where the primary issues lie.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16478a3e",
   "metadata": {},
   "source": [
    "### 2.1 Coherence Evaluator\n",
    "CoherenceEvaluator measures the logical and orderly presentation of ideas in a response, allowing the reader to easily follow and understand the writer's train of thought. A coherent response directly addresses the question with clear connections between sentences and paragraphs, using appropriate transitions and a logical sequence of ideas. Higher scores mean better coherence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f105f2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coherence': 4.0,\n",
       " 'gpt_coherence': 4.0,\n",
       " 'coherence_reason': 'The RESPONSE is coherent because it directly answers the QUERY with a clear and logical statement, making it easy to understand.',\n",
       " 'coherence_result': 'pass',\n",
       " 'coherence_threshold': 3}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.ai.evaluation import CoherenceEvaluator\n",
    "\n",
    "coherence = CoherenceEvaluator(model_config=model_config, threshold=3)\n",
    "coherence(\n",
    "    query=\"Is Marie Curie is born in Paris?\", \n",
    "    response=\"No, Marie Curie is born in Warsaw.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604804fb",
   "metadata": {},
   "source": [
    "### 2.2 Fluency Evaluator\n",
    "luencyEvaluatormeasures the effectiveness and clarity of written communication, focusing on grammatical accuracy, vocabulary range, sentence complexity, coherence, and overall readability. It assesses how smoothly ideas are conveyed and how easily the reader can understand the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f56acd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fluency': 2.0,\n",
       " 'gpt_fluency': 2.0,\n",
       " 'fluency_reason': 'The response should receive a Score of 2 because it communicates a simple idea with a grammatical error and limited vocabulary, fitting the definition of Basic Fluency.',\n",
       " 'fluency_result': 'fail',\n",
       " 'fluency_threshold': 3}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.ai.evaluation import FluencyEvaluator\n",
    "\n",
    "fluency = FluencyEvaluator(model_config=model_config, threshold=3)\n",
    "fluency(\n",
    "    response=\"No, Marie Curie is born in Warsaw.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a17578",
   "metadata": {},
   "source": [
    "### 2.3 Question-Answering Composite Evaluator\n",
    "QAEvaluator measures comprehensively various aspects in a question-answering scenario - including Relevance, Groundedness, Fluency, Coherence, Similarity, and F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6920cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1_score': 0.631578947368421,\n",
       " 'f1_result': 'pass',\n",
       " 'f1_threshold': 3,\n",
       " 'similarity': 5.0,\n",
       " 'gpt_similarity': 5.0,\n",
       " 'similarity_result': 'pass',\n",
       " 'similarity_threshold': 3,\n",
       " 'relevance': 5.0,\n",
       " 'gpt_relevance': 5.0,\n",
       " 'relevance_reason': 'The response accurately and completely answers the query, providing both the correct birthplace and additional clarification, making it comprehensive with insights.',\n",
       " 'relevance_result': 'pass',\n",
       " 'relevance_threshold': 3,\n",
       " 'fluency': 3.0,\n",
       " 'gpt_fluency': 3.0,\n",
       " 'fluency_reason': 'The response is clear and grammatically correct, with adequate vocabulary. It conveys the intended message effectively, but the sentence structure is simple and lacks complexity, which aligns with Competent Fluency.',\n",
       " 'fluency_result': 'pass',\n",
       " 'fluency_threshold': 3,\n",
       " 'coherence': 4.0,\n",
       " 'gpt_coherence': 4.0,\n",
       " 'coherence_reason': 'The response is coherent and provides the correct answer to the query, but it includes an unnecessary comparison that slightly affects the logical flow.',\n",
       " 'coherence_result': 'pass',\n",
       " 'coherence_threshold': 3,\n",
       " 'groundedness': 3.0,\n",
       " 'gpt_groundedness': 3.0,\n",
       " 'groundedness_reason': 'The RESPONSE correctly answers the QUERY but does not use the CONTEXT to support the answer. It provides accurate information but lacks grounding in the provided CONTEXT.',\n",
       " 'groundedness_result': 'pass',\n",
       " 'groundedness_threshold': 3}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.ai.evaluation import QAEvaluator\n",
    "\n",
    "qa_eval = QAEvaluator(model_config=model_config, threshold=3)\n",
    "qa_eval(\n",
    "    query=\"Where was Marie Curie born?\", \n",
    "    context=\"Background: 1. Marie Curie was a chemist. 2. Marie Curie was born on November 7, 1867. 3. Marie Curie is a French scientist.\",\n",
    "    response=\"According to wikipedia, Marie Curie was not born in Paris but in Warsaw.\",\n",
    "    ground_truth=\"Marie Curie was born in Warsaw.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd71b234",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3.  Retrieval Augmented Generation (RAG) Evaluators\n",
    "\n",
    "A retrieval-augmented generation (RAG) system tries to generate the most relevant answer consistent with grounding documents in response to a user's query.  This requires it to _retrieve_ documents that provide grounding context, and _generate_ responses that are relevance, consistent with grounding data, and complete.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a2c47f",
   "metadata": {},
   "source": [
    "### 3.1 Retrieval Evaluator\n",
    "RetrievalEvaluator measures the textual quality of retrieval results with an LLM without requiring ground truth. This metric focuses on how relevant the context chunks (encoded as a string) are to address a query and how the most relevant context chunks are surfaced at the top of the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65955c20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'retrieval': 5.0,\n",
       " 'gpt_retrieval': 5.0,\n",
       " 'retrieval_reason': 'The context contains the exact answer to the query at the top, with no external knowledge bias introduced, making it highly relevant and well-ranked.',\n",
       " 'retrieval_result': 'pass',\n",
       " 'retrieval_threshold': 3}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.ai.evaluation import RetrievalEvaluator\n",
    "\n",
    "retrieval = RetrievalEvaluator(model_config=model_config, threshold=3)\n",
    "retrieval(\n",
    "    query=\"Where was Marie Curie born?\", \n",
    "    context=\"Background: 1. Marie Curie was born in Warsaw. 2. Marie Curie was born on November 7, 1867. 3. Marie Curie is a French scientist. \",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c920eff3",
   "metadata": {},
   "source": [
    "\n",
    "### 3.2 Groundedness Evaluator \n",
    "GroundednessEvaluator measures how well the generated response aligns with the given context (grounding source) and doesn't fabricate content outside of it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eda150a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'groundedness': 5.0,\n",
       " 'gpt_groundedness': 5.0,\n",
       " 'groundedness_reason': 'The response is fully grounded in the context, accurately and completely answering the query without adding extraneous information.',\n",
       " 'groundedness_result': 'pass',\n",
       " 'groundedness_threshold': 3}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.ai.evaluation import GroundednessEvaluator\n",
    "\n",
    "groundedness = GroundednessEvaluator(model_config=model_config, threshold=3)\n",
    "groundedness(\n",
    "    query=\"Is Marie Curie is born in Paris?\", \n",
    "    context=\"Background: 1. Marie Curie is born on November 7, 1867. 2. Marie Curie is born in Warsaw.\",\n",
    "    response=\"No, Marie Curie is born in Warsaw.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f433c969",
   "metadata": {},
   "source": [
    "### 3.3 Relevance Evaluator\n",
    "\n",
    "RelevanceEvaluator measures how effectively a response addresses a query. It assesses the accuracy, completeness, and direct relevance of the response based solely on the given query. Higher scores mean better relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44116930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'relevance': 4.0,\n",
       " 'gpt_relevance': 4.0,\n",
       " 'relevance_reason': 'The RESPONSE accurately answers the QUERY by stating that Marie Curie was born in Warsaw, which is correct and directly relevant to the question asked.',\n",
       " 'relevance_result': 'pass',\n",
       " 'relevance_threshold': 3}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    \"relevance\": 4.0,\n",
    "    \"gpt_relevance\": 4.0, \n",
    "    \"relevance_reason\": \"The RESPONSE accurately answers the QUERY by stating that Marie Curie was born in Warsaw, which is correct and directly relevant to the question asked.\",\n",
    "    \"relevance_result\": \"pass\", \n",
    "    \"relevance_threshold\": 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da91363c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'coherence': 4.0,\n",
      " 'coherence_reason': 'The RESPONSE is coherent because it directly answers the '\n",
      "                     'QUERY with a clear and logical sentence.',\n",
      " 'coherence_result': 'pass',\n",
      " 'coherence_threshold': 3,\n",
      " 'gpt_coherence': 4.0}\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.evaluation import CoherenceEvaluator\n",
    "coherence_evaluator = CoherenceEvaluator(model_config)\n",
    "\n",
    "result = coherence_evaluator(\n",
    "    query=\"What is the capital of Japan?\",\n",
    "    response=\"The capital of Japan is Tokyo.\"\n",
    ")\n",
    "\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2179ed8f",
   "metadata": {},
   "source": [
    "### 3.4 Response Completeness Evaluator\n",
    "\n",
    "ResponseCompletenessEvaluator that captures the recall aspect of response alignment with the expected response. This is complementary to GroundednessEvaluator which captures the precision aspect of response alignment with the grounding source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25a3b14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class ResponseCompletenessEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'response_completeness': 1,\n",
       " 'response_completeness_result': 'fail',\n",
       " 'response_completeness_threshold': 3,\n",
       " 'response_completeness_reason': \"The response completely misses the key information about the CEO's compensation package discussed in the shareholder meeting, which is the focus of the ground truth.\"}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.ai.evaluation import ResponseCompletenessEvaluator\n",
    "\n",
    "response_completeness = ResponseCompletenessEvaluator(model_config=model_config, threshold=3)\n",
    "response_completeness(\n",
    "    response=\"Based on the retrieved documents, the shareholder meeting discussed the operational efficiency of the company and financing options.\",\n",
    "    ground_truth=\"The shareholder meeting discussed the compensation package of the company CEO.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56667c74",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Textual Similarity Evaluators\n",
    "\n",
    "\n",
    "These evaluators compare how closely the textual response generated by your AI system matches the response you would expect, typically called the \"ground truth\".\n",
    "- The SimilarityEvaluator uses an \"LLM-as-Judge\" (AI-assisted evaluation) approach to score the metric.\n",
    "- The F1 Score, BLEU, GLEU, ROUGE and METEOR evaluators (NLP-based) use a mathematical approach to score the metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1872f9",
   "metadata": {},
   "source": [
    "### 4.1 Similarity Evaluator\n",
    "SimilarityEvaluator measures the degrees of semantic similarity between the generated text and its ground truth with respect to a query. Compared to other text-similarity metrics that require ground truths, this metric focuses on semantics of a response (instead of simple overlap in tokens or n-grams) and also considers the broader context of a query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe271d9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'similarity': 5.0,\n",
       " 'gpt_similarity': 5.0,\n",
       " 'similarity_result': 'pass',\n",
       " 'similarity_threshold': 3}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.ai.evaluation import SimilarityEvaluator\n",
    "\n",
    "similarity = SimilarityEvaluator(model_config=model_config, threshold=3)\n",
    "similarity(\n",
    "    query=\"Is Marie Curie is born in Paris?\", \n",
    "    response=\"According to wikipedia, Marie Curie was not born in Paris but in Warsaw.\",\n",
    "    ground_truth=\"Marie Curie was born in Warsaw.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1e7260",
   "metadata": {},
   "source": [
    "### 4.2 F1 Score\n",
    "F1ScoreEvaluator measures the similarity by shared tokens between the generated text and the ground truth, focusing on both precision and recall. The F1-score computes the ratio of the number of shared words between the model generation and the ground truth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588f292f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1_score': 0.631578947368421, 'f1_result': 'fail', 'f1_threshold': 0.5}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.ai.evaluation import F1ScoreEvaluator\n",
    "\n",
    "f1_score = F1ScoreEvaluator(threshold=0.5)\n",
    "f1_score(\n",
    "    response=\"According to wikipedia, Marie Curie was not born in Paris but in Warsaw.\",\n",
    "    ground_truth=\"Marie Curie was born in Warsaw.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16081012",
   "metadata": {},
   "source": [
    "### 4.3 BLEU Score\n",
    "BleuScoreEvaluator computes the BLEU (Bilingual Evaluation Understudy) score commonly used in natural language processing (NLP) and machine translation. It measures how closely the generated text matches the reference text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4580ed7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu_score': 0.1550967560878879,\n",
       " 'bleu_result': 'fail',\n",
       " 'bleu_threshold': 0.3}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.ai.evaluation import BleuScoreEvaluator\n",
    "\n",
    "bleu_score = BleuScoreEvaluator(threshold=0.3)\n",
    "bleu_score(\n",
    "    response=\"According to wikipedia, Marie Curie was not born in Paris but in Warsaw.\",\n",
    "    ground_truth=\"Marie Curie was born in Warsaw.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cd0a01",
   "metadata": {},
   "source": [
    "### 4.4 GLEU Score\n",
    "\n",
    "GleuScoreEvaluator computes the GLEU (Google-BLEU) score. It measures the similarity by shared n-grams between the generated text and ground truth, similar to the BLEU score, focusing on both precision and recall. But it addresses the drawbacks of the BLEU score using a per-sentence reward objective. The numerical score is a 0-1 float and a higher score is better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7159326d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gleu_score': 0.25925925925925924,\n",
       " 'gleu_result': 'fail',\n",
       " 'gleu_threshold': 0.2}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.ai.evaluation import GleuScoreEvaluator\n",
    "\n",
    "\n",
    "gleu_score = GleuScoreEvaluator(threshold=0.2)\n",
    "gleu_score(\n",
    "    response=\"According to wikipedia, Marie Curie was not born in Paris but in Warsaw.\",\n",
    "    ground_truth=\"Marie Curie was born in Warsaw.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4477569f",
   "metadata": {},
   "source": [
    "### 4.5 ROUGE Score\n",
    "RougeScoreEvaluator computes the ROUGE (Recall-Oriented Understudy for Gisting Evaluation) scores, a set of metrics used to evaluate automatic summarization and machine translation. It measures the overlap between generated text and reference summaries. The numerical score is a 0-1 float and a higher score is better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12b716a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge_precision': 0.46153846153846156,\n",
       " 'rouge_recall': 1.0,\n",
       " 'rouge_f1_score': 0.631578947368421,\n",
       " 'rouge_precision_result': 'fail',\n",
       " 'rouge_recall_result': 'pass',\n",
       " 'rouge_f1_score_result': 'pass',\n",
       " 'rouge_precision_threshold': 0.6,\n",
       " 'rouge_recall_threshold': 0.5,\n",
       " 'rouge_f1_score_threshold': 0.55}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.ai.evaluation import RougeScoreEvaluator, RougeType\n",
    "\n",
    "rouge = RougeScoreEvaluator(rouge_type=RougeType.ROUGE_L, precision_threshold=0.6, recall_threshold=0.5, f1_score_threshold=0.55) \n",
    "rouge(\n",
    "    response=\"According to wikipedia, Marie Curie was not born in Paris but in Warsaw.\",\n",
    "    ground_truth=\"Marie Curie was born in Warsaw.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62c3d9d",
   "metadata": {},
   "source": [
    "### 4.6 METEOR Score\n",
    "MeteorScoreEvaluator measures the similarity by shared n-grams between the generated text and the ground truth, similar to the BLEU score, focusing on precision and recall. But it addresses limitations of other metrics like the BLEU score by considering synonyms, stemming, and paraphrasing for content alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e971963d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'meteor_score': 0.8621140763997908,\n",
       " 'meteor_result': 'fail',\n",
       " 'meteor_threshold': 0.9}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.ai.evaluation import MeteorScoreEvaluator\n",
    "\n",
    "meteor_score = MeteorScoreEvaluator(threshold=0.9)\n",
    "meteor_score(\n",
    "    response=\"According to wikipedia, Marie Curie was not born in Paris but in Warsaw.\",\n",
    "    ground_truth=\"Marie Curie was born in Warsaw.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d223752",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Explore Custom Evaluators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9843c29",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 5.1 Code-Based Evaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5679169b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "{'score': False}\n",
      "{'score': True}\n"
     ]
    }
   ],
   "source": [
    "# Custom evaluator as a function to calculate response length\n",
    "def response_length(response, **kwargs):\n",
    "    return len(response)\n",
    "\n",
    "# Custom class based evaluator to check for blocked words\n",
    "class BlocklistEvaluator:\n",
    "    def __init__(self, blocklist):\n",
    "        self._blocklist = blocklist\n",
    "\n",
    "    def __call__(self, *, answer: str, **kwargs):\n",
    "        contains_block_word = any(word in answer for word in self._blocklist)\n",
    "        return {\"score\": contains_block_word}\n",
    "\n",
    "blocklist_evaluator = BlocklistEvaluator(blocklist=[\"bad\", \"worst\", \"terrible\"])\n",
    "\n",
    "# Test custom evaluator 1\n",
    "result = response_length(\"The capital of Japan is Tokyo.\")\n",
    "print(result)\n",
    "\n",
    "# Test custom evaluator 2\n",
    "result = blocklist_evaluator(answer=\"The capital of Japan is Tokyo.\")\n",
    "print(result)\n",
    "\n",
    "# Test custom evaluator 3\n",
    "result = blocklist_evaluator(answer=\"This is a bad idea.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0202d615",
   "metadata": {},
   "source": [
    "### 5.2 Prompt-Based Evaluator\n",
    "To build your own prompt-based large language model evaluator or AI-assisted annotator, you can create a custom evaluator based on a Prompty file. This is a file with a `.prompty` extension that adheres to the [Prompty specification](https://prompty.io) - defining a prompt asset that contains both model configuration and content template, for your prompt-based evaluator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc9ef69",
   "metadata": {},
   "source": [
    "**STEP ONE: Create a Prompty file**\n",
    "\n",
    "Explore the [02-friendliness.prompty](02-friendliness.prompty) file in the current folder to see what that looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2284bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP TWO: Define the evaluator class - this loads the prompty file and uses it as a \"instruction prompt\" to guide the Judge LLM to grade the app response\n",
    "\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "from promptflow.client import load_flow\n",
    "\n",
    "\n",
    "class FriendlinessEvaluator:\n",
    "    def __init__(self, model_config):\n",
    "        current_dir = os.getcwd()\n",
    "        prompty_path = os.path.join(current_dir, \"02-friendliness.prompty\")\n",
    "        self._flow = load_flow(source=prompty_path, model={\"configuration\": model_config})\n",
    "\n",
    "    def __call__(self, *, response: str, **kwargs):\n",
    "        llm_response = self._flow(response=response)\n",
    "        try:\n",
    "            response = json.loads(llm_response)\n",
    "        except Exception as ex:\n",
    "            response = llm_response\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cad44a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reason': 'The response is mostly unfriendly, as it is defensive and lacks '\n",
      "           'warmth or empathy.',\n",
      " 'score': 2}\n"
     ]
    }
   ],
   "source": [
    "# STEP THREE: Run the evaluator - give it a response to grade\n",
    "\n",
    "friendliness_eval = FriendlinessEvaluator(model_config)\n",
    "\n",
    "friendliness_score = friendliness_eval(response=\"I will not apologize for my behavior!\")\n",
    "pprint(friendliness_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2107224d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Run Multiple Evaluators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a70075c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class ContentSafetyEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class ViolenceEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class SexualEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class SelfHarmEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class HateUnfairnessEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "[2025-05-15 21:38:05 +0000][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-05-15 21:38:05 +0000][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-05-15 21:38:05 +0000][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_fluency_20250515_213805_811824, log path: /home/vscode/.promptflow/.runs/azure_ai_evaluation_evaluators_fluency_20250515_213805_811824/logs.txt\n",
      "[2025-05-15 21:38:05 +0000][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_relevance_20250515_213805_811216, log path: /home/vscode/.promptflow/.runs/azure_ai_evaluation_evaluators_relevance_20250515_213805_811216/logs.txt\n",
      "[2025-05-15 21:38:05 +0000][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-05-15 21:38:05 +0000][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-05-15 21:38:05 +0000][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-05-15 21:38:06 +0000][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-05-15 21:38:06 +0000][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_groundedness_20250515_213805_811551, log path: /home/vscode/.promptflow/.runs/azure_ai_evaluation_evaluators_groundedness_20250515_213805_811551/logs.txt\n",
      "[2025-05-15 21:38:06 +0000][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_similarity_20250515_213805_817765, log path: /home/vscode/.promptflow/.runs/azure_ai_evaluation_evaluators_similarity_20250515_213805_817765/logs.txt\n",
      "[2025-05-15 21:38:06 +0000][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_content_safety_20250515_213805_806343, log path: /home/vscode/.promptflow/.runs/azure_ai_evaluation_evaluators_content_safety_20250515_213805_806343/logs.txt\n",
      "[2025-05-15 21:38:06 +0000][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_coherence_20250515_213805_807734, log path: /home/vscode/.promptflow/.runs/azure_ai_evaluation_evaluators_coherence_20250515_213805_807734/logs.txt\n",
      "[2025-05-15 21:38:07 +0000][promptflow._sdk._orchestrator.run_submitter][WARNING] - 5 out of 5 runs failed in batch run.\n",
      " Please check out /home/vscode/.promptflow/.runs/azure_ai_evaluation_evaluators_similarity_20250515_213805_817765 for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-15 21:38:06 +0000   98702 execution.bulk     INFO     Current thread is not main thread, skip signal handler registration in BatchEngine.\n",
      "2025-05-15 21:38:06 +0000   98702 execution.bulk     INFO     Finished 5 / 5 lines.\n",
      "2025-05-15 21:38:06 +0000   98702 execution.bulk     INFO     Average execution time for completed lines: 0.02 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-05-15 21:38:06 +0000   98702 execution          ERROR    5/5 flow run failed, indexes: [2,0,3,1,4], exception of index 2: (UserError) SimilarityEvaluator: Either 'conversation' or individual inputs must be provided.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"azure_ai_evaluation_evaluators_similarity_20250515_213805_817765\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-05-15 21:38:05.830371+00:00\"\n",
      "Duration: \"0:00:01.509708\"\n",
      "Output path: \"/home/vscode/.promptflow/.runs/azure_ai_evaluation_evaluators_similarity_20250515_213805_817765\"\n",
      "\n",
      "2025-05-15 21:38:08 +0000   98702 execution.bulk     INFO     Finished 1 / 5 lines.\n",
      "2025-05-15 21:38:08 +0000   98702 execution.bulk     INFO     Average execution time for completed lines: 2.5 seconds. Estimated time for incomplete lines: 10.0 seconds.\n",
      "2025-05-15 21:38:08 +0000   98702 execution.bulk     INFO     Finished 2 / 5 lines.\n",
      "2025-05-15 21:38:08 +0000   98702 execution.bulk     INFO     Average execution time for completed lines: 1.24 seconds. Estimated time for incomplete lines: 3.72 seconds.\n",
      "2025-05-15 21:38:08 +0000   98702 execution.bulk     INFO     Finished 2 / 5 lines.\n",
      "2025-05-15 21:38:08 +0000   98702 execution.bulk     INFO     Average execution time for completed lines: 1.27 seconds. Estimated time for incomplete lines: 3.81 seconds.\n",
      "2025-05-15 21:38:08 +0000   98702 execution.bulk     INFO     Finished 3 / 5 lines.\n",
      "2025-05-15 21:38:08 +0000   98702 execution.bulk     INFO     Average execution time for completed lines: 0.88 seconds. Estimated time for incomplete lines: 1.76 seconds.\n",
      "2025-05-15 21:38:08 +0000   98702 execution.bulk     INFO     Finished 3 / 5 lines.\n",
      "2025-05-15 21:38:08 +0000   98702 execution.bulk     INFO     Average execution time for completed lines: 0.87 seconds. Estimated time for incomplete lines: 1.74 seconds.\n",
      "2025-05-15 21:38:08 +0000   98702 execution.bulk     INFO     Finished 1 / 5 lines.\n",
      "2025-05-15 21:38:08 +0000   98702 execution.bulk     INFO     Average execution time for completed lines: 2.63 seconds. Estimated time for incomplete lines: 10.52 seconds.\n",
      "2025-05-15 21:38:08 +0000   98702 execution.bulk     INFO     Finished 4 / 5 lines.\n",
      "2025-05-15 21:38:08 +0000   98702 execution.bulk     INFO     Average execution time for completed lines: 0.68 seconds. Estimated time for incomplete lines: 0.68 seconds.\n",
      "2025-05-15 21:38:08 +0000   98702 execution.bulk     INFO     Finished 2 / 5 lines.\n",
      "2025-05-15 21:38:08 +0000   98702 execution.bulk     INFO     Average execution time for completed lines: 1.33 seconds. Estimated time for incomplete lines: 3.99 seconds.\n",
      "2025-05-15 21:38:08 +0000   98702 execution.bulk     INFO     Finished 3 / 5 lines.\n",
      "2025-05-15 21:38:08 +0000   98702 execution.bulk     INFO     Finished 1 / 5 lines.\n",
      "2025-05-15 21:38:08 +0000   98702 execution.bulk     INFO     Average execution time for completed lines: 2.78 seconds. Estimated time for incomplete lines: 11.12 seconds.\n",
      "2025-05-15 21:38:08 +0000   98702 execution.bulk     INFO     Average execution time for completed lines: 0.91 seconds. Estimated time for incomplete lines: 1.82 seconds.\n",
      "2025-05-15 21:38:09 +0000   98702 execution.bulk     INFO     Finished 4 / 5 lines.\n",
      "2025-05-15 21:38:09 +0000   98702 execution.bulk     INFO     Average execution time for completed lines: 0.72 seconds. Estimated time for incomplete lines: 0.72 seconds.\n",
      "2025-05-15 21:38:09 +0000   98702 execution.bulk     INFO     Finished 4 / 5 lines.\n",
      "2025-05-15 21:38:09 +0000   98702 execution.bulk     INFO     Average execution time for completed lines: 0.73 seconds. Estimated time for incomplete lines: 0.73 seconds.\n",
      "2025-05-15 21:38:09 +0000   98702 execution.bulk     INFO     Finished 5 / 5 lines.\n",
      "2025-05-15 21:38:09 +0000   98702 execution.bulk     INFO     Average execution time for completed lines: 0.6 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-05-15 21:38:06 +0000   98702 execution.bulk     INFO     Current thread is not main thread, skip signal handler registration in BatchEngine.\n",
      "2025-05-15 21:38:08 +0000   98702 execution.bulk     INFO     Finished 2 / 5 lines.\n",
      "2025-05-15 21:38:08 +0000   98702 execution.bulk     INFO     Average execution time for completed lines: 1.24 seconds. Estimated time for incomplete lines: 3.72 seconds.\n",
      "2025-05-15 21:38:08 +0000   98702 execution.bulk     INFO     Finished 3 / 5 lines.\n",
      "2025-05-15 21:38:08 +0000   98702 execution.bulk     INFO     Average execution time for completed lines: 0.87 seconds. Estimated time for incomplete lines: 1.74 seconds.\n",
      "2025-05-15 21:38:09 +0000   98702 execution.bulk     INFO     Finished 4 / 5 lines.\n",
      "2025-05-15 21:38:09 +0000   98702 execution.bulk     INFO     Average execution time for completed lines: 0.72 seconds. Estimated time for incomplete lines: 0.72 seconds.\n",
      "2025-05-15 21:38:09 +0000   98702 execution.bulk     INFO     Finished 5 / 5 lines.\n",
      "2025-05-15 21:38:09 +0000   98702 execution.bulk     INFO     Average execution time for completed lines: 0.6 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"azure_ai_evaluation_evaluators_relevance_20250515_213805_811216\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-05-15 21:38:05.832535+00:00\"\n",
      "Duration: \"0:00:03.435944\"\n",
      "Output path: \"/home/vscode/.promptflow/.runs/azure_ai_evaluation_evaluators_relevance_20250515_213805_811216\"\n",
      "\n",
      "2025-05-15 21:38:09 +0000   98702 execution.bulk     INFO     Finished 5 / 5 lines.\n",
      "2025-05-15 21:38:09 +0000   98702 execution.bulk     INFO     Average execution time for completed lines: 0.65 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-05-15 21:38:09 +0000   98702 execution.bulk     INFO     Finished 2 / 5 lines.\n",
      "2025-05-15 21:38:09 +0000   98702 execution.bulk     INFO     Average execution time for completed lines: 1.63 seconds. Estimated time for incomplete lines: 4.89 seconds.\n",
      "2025-05-15 21:38:09 +0000   98702 execution.bulk     INFO     Finished 3 / 5 lines.\n",
      "2025-05-15 21:38:09 +0000   98702 execution.bulk     INFO     Average execution time for completed lines: 1.09 seconds. Estimated time for incomplete lines: 2.18 seconds.\n",
      "2025-05-15 21:38:09 +0000   98702 execution.bulk     INFO     Finished 4 / 5 lines.\n",
      "2025-05-15 21:38:09 +0000   98702 execution.bulk     INFO     Average execution time for completed lines: 0.84 seconds. Estimated time for incomplete lines: 0.84 seconds.\n",
      "2025-05-15 21:38:09 +0000   98702 execution.bulk     INFO     Finished 5 / 5 lines.\n",
      "2025-05-15 21:38:09 +0000   98702 execution.bulk     INFO     Average execution time for completed lines: 0.73 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-05-15 21:38:06 +0000   98702 execution.bulk     INFO     Current thread is not main thread, skip signal handler registration in BatchEngine.\n",
      "2025-05-15 21:38:08 +0000   98702 execution.bulk     INFO     Finished 1 / 5 lines.\n",
      "2025-05-15 21:38:08 +0000   98702 execution.bulk     INFO     Average execution time for completed lines: 2.5 seconds. Estimated time for incomplete lines: 10.0 seconds.\n",
      "2025-05-15 21:38:08 +0000   98702 execution.bulk     INFO     Finished 2 / 5 lines.\n",
      "2025-05-15 21:38:08 +0000   98702 execution.bulk     INFO     Average execution time for completed lines: 1.27 seconds. Estimated time for incomplete lines: 3.81 seconds.\n",
      "2025-05-15 21:38:08 +0000   98702 execution.bulk     INFO     Finished 3 / 5 lines.\n",
      "2025-05-15 21:38:08 +0000   98702 execution.bulk     INFO     Average execution time for completed lines: 0.88 seconds. Estimated time for incomplete lines: 1.76 seconds.\n",
      "2025-05-15 21:38:08 +0000   98702 execution.bulk     INFO     Finished 4 / 5 lines.\n",
      "2025-05-15 21:38:08 +0000   98702 execution.bulk     INFO     Average execution time for completed lines: 0.68 seconds. Estimated time for incomplete lines: 0.68 seconds.\n",
      "2025-05-15 21:38:09 +0000   98702 execution.bulk     INFO     Finished 5 / 5 lines.\n",
      "2025-05-15 21:38:09 +0000   98702 execution.bulk     INFO     Average execution time for completed lines: 0.65 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"azure_ai_evaluation_evaluators_fluency_20250515_213805_811824\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-05-15 21:38:05.837977+00:00\"\n",
      "Duration: \"0:00:04.373333\"\n",
      "Output path: \"/home/vscode/.promptflow/.runs/azure_ai_evaluation_evaluators_fluency_20250515_213805_811824\"\n",
      "\n",
      "2025-05-15 21:38:06 +0000   98702 execution.bulk     INFO     Current thread is not main thread, skip signal handler registration in BatchEngine.\n",
      "2025-05-15 21:38:08 +0000   98702 execution.bulk     INFO     Finished 1 / 5 lines.\n",
      "2025-05-15 21:38:08 +0000   98702 execution.bulk     INFO     Average execution time for completed lines: 2.63 seconds. Estimated time for incomplete lines: 10.52 seconds.\n",
      "2025-05-15 21:38:08 +0000   98702 execution.bulk     INFO     Finished 2 / 5 lines.\n",
      "2025-05-15 21:38:08 +0000   98702 execution.bulk     INFO     Average execution time for completed lines: 1.33 seconds. Estimated time for incomplete lines: 3.99 seconds.\n",
      "2025-05-15 21:38:08 +0000   98702 execution.bulk     INFO     Finished 3 / 5 lines.\n",
      "2025-05-15 21:38:08 +0000   98702 execution.bulk     INFO     Average execution time for completed lines: 0.91 seconds. Estimated time for incomplete lines: 1.82 seconds.\n",
      "2025-05-15 21:38:09 +0000   98702 execution.bulk     INFO     Finished 4 / 5 lines.\n",
      "2025-05-15 21:38:09 +0000   98702 execution.bulk     INFO     Average execution time for completed lines: 0.73 seconds. Estimated time for incomplete lines: 0.73 seconds.\n",
      "2025-05-15 21:38:09 +0000   98702 execution.bulk     INFO     Finished 5 / 5 lines.\n",
      "2025-05-15 21:38:09 +0000   98702 execution.bulk     INFO     Average execution time for completed lines: 0.73 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"azure_ai_evaluation_evaluators_coherence_20250515_213805_807734\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-05-15 21:38:05.831904+00:00\"\n",
      "Duration: \"0:00:04.446005\"\n",
      "Output path: \"/home/vscode/.promptflow/.runs/azure_ai_evaluation_evaluators_coherence_20250515_213805_807734\"\n",
      "\n",
      "2025-05-15 21:38:10 +0000   98702 execution.bulk     INFO     Finished 5 / 5 lines.\n",
      "2025-05-15 21:38:10 +0000   98702 execution.bulk     INFO     Average execution time for completed lines: 0.86 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-05-15 21:38:06 +0000   98702 execution.bulk     INFO     Current thread is not main thread, skip signal handler registration in BatchEngine.\n",
      "2025-05-15 21:38:08 +0000   98702 execution.bulk     INFO     Finished 1 / 5 lines.\n",
      "2025-05-15 21:38:08 +0000   98702 execution.bulk     INFO     Average execution time for completed lines: 2.78 seconds. Estimated time for incomplete lines: 11.12 seconds.\n",
      "2025-05-15 21:38:09 +0000   98702 execution.bulk     INFO     Finished 2 / 5 lines.\n",
      "2025-05-15 21:38:09 +0000   98702 execution.bulk     INFO     Average execution time for completed lines: 1.63 seconds. Estimated time for incomplete lines: 4.89 seconds.\n",
      "2025-05-15 21:38:09 +0000   98702 execution.bulk     INFO     Finished 3 / 5 lines.\n",
      "2025-05-15 21:38:09 +0000   98702 execution.bulk     INFO     Average execution time for completed lines: 1.09 seconds. Estimated time for incomplete lines: 2.18 seconds.\n",
      "2025-05-15 21:38:09 +0000   98702 execution.bulk     INFO     Finished 4 / 5 lines.\n",
      "2025-05-15 21:38:09 +0000   98702 execution.bulk     INFO     Average execution time for completed lines: 0.84 seconds. Estimated time for incomplete lines: 0.84 seconds.\n",
      "2025-05-15 21:38:10 +0000   98702 execution.bulk     INFO     Finished 5 / 5 lines.\n",
      "2025-05-15 21:38:10 +0000   98702 execution.bulk     INFO     Average execution time for completed lines: 0.86 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"azure_ai_evaluation_evaluators_groundedness_20250515_213805_811551\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-05-15 21:38:05.823898+00:00\"\n",
      "Duration: \"0:00:05.388193\"\n",
      "Output path: \"/home/vscode/.promptflow/.runs/azure_ai_evaluation_evaluators_groundedness_20250515_213805_811551\"\n",
      "\n",
      "2025-05-15 21:38:55 +0000   98702 execution.bulk     INFO     Finished 5 / 5 lines.\n",
      "2025-05-15 21:38:55 +0000   98702 execution.bulk     INFO     Average execution time for completed lines: 9.87 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-05-15 21:38:06 +0000   98702 execution.bulk     INFO     Current thread is not main thread, skip signal handler registration in BatchEngine.\n",
      "2025-05-15 21:38:55 +0000   98702 execution.bulk     INFO     Finished 5 / 5 lines.\n",
      "2025-05-15 21:38:55 +0000   98702 execution.bulk     INFO     Average execution time for completed lines: 9.87 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"azure_ai_evaluation_evaluators_content_safety_20250515_213805_806343\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-05-15 21:38:05.817126+00:00\"\n",
      "Duration: \"0:00:49.733499\"\n",
      "Output path: \"/home/vscode/.promptflow/.runs/azure_ai_evaluation_evaluators_content_safety_20250515_213805_806343\"\n",
      "\n",
      "======= Combined Run Summary (Per Evaluator) =======\n",
      "\n",
      "{\n",
      "    \"content_safety\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:49.733499\",\n",
      "        \"completed_lines\": 5,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": \"/home/vscode/.promptflow/.runs/azure_ai_evaluation_evaluators_content_safety_20250515_213805_806343\"\n",
      "    },\n",
      "    \"coherence\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:04.446005\",\n",
      "        \"completed_lines\": 5,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": \"/home/vscode/.promptflow/.runs/azure_ai_evaluation_evaluators_coherence_20250515_213805_807734\"\n",
      "    },\n",
      "    \"relevance\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:03.435944\",\n",
      "        \"completed_lines\": 5,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": \"/home/vscode/.promptflow/.runs/azure_ai_evaluation_evaluators_relevance_20250515_213805_811216\"\n",
      "    },\n",
      "    \"groundedness\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:05.388193\",\n",
      "        \"completed_lines\": 5,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": \"/home/vscode/.promptflow/.runs/azure_ai_evaluation_evaluators_groundedness_20250515_213805_811551\"\n",
      "    },\n",
      "    \"fluency\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:04.373333\",\n",
      "        \"completed_lines\": 5,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": \"/home/vscode/.promptflow/.runs/azure_ai_evaluation_evaluators_fluency_20250515_213805_811824\"\n",
      "    },\n",
      "    \"similarity\": {\n",
      "        \"status\": \"Completed with Errors\",\n",
      "        \"duration\": \"0:00:01.509708\",\n",
      "        \"completed_lines\": 0,\n",
      "        \"failed_lines\": 5,\n",
      "        \"log_path\": \"/home/vscode/.promptflow/.runs/azure_ai_evaluation_evaluators_similarity_20250515_213805_817765\"\n",
      "    }\n",
      "}\n",
      "\n",
      "====================================================\n",
      "\n",
      "Evaluation results saved to \"/workspaces/BUILD25-LAB334/labs/02-quality-evaluators.results.json\".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.evaluation import evaluate\n",
    "from azure.ai.evaluation import (\n",
    "    ContentSafetyEvaluator,\n",
    "    RelevanceEvaluator,\n",
    "    CoherenceEvaluator,\n",
    "    GroundednessEvaluator,\n",
    "    FluencyEvaluator,\n",
    "    SimilarityEvaluator,\n",
    ")\n",
    "\n",
    "# Create evaluators\n",
    "content_safety_evaluator = ContentSafetyEvaluator( azure_ai_project=azure_ai_project, credential=credential)\n",
    "relevance_evaluator = RelevanceEvaluator(model_config)\n",
    "coherence_evaluator = CoherenceEvaluator(model_config)\n",
    "groundedness_evaluator = GroundednessEvaluator(model_config)\n",
    "fluency_evaluator = FluencyEvaluator(model_config)\n",
    "similarity_evaluator = SimilarityEvaluator(model_config)\n",
    "\n",
    "\n",
    "result = evaluate(\n",
    "    data=\"00-data/02-data.jsonl\",\n",
    "    evaluators={\n",
    "        \"content_safety\": content_safety_evaluator,\n",
    "        \"coherence\": coherence_evaluator,\n",
    "        \"relevance\": relevance_evaluator,\n",
    "        \"groundedness\": groundedness_evaluator,\n",
    "        \"fluency\": fluency_evaluator,\n",
    "        \"similarity\": similarity_evaluator,\n",
    "    },\n",
    "    evaluation_name=\"02-quality-evaluators\",\n",
    "    # column mapping\n",
    "    evaluator_config={\n",
    "        \"content_safety\": {\"column_mapping\": {\"query\": \"${data.query}\", \"response\": \"${data.response}\"}},\n",
    "        \"coherence\": {\"column_mapping\": {\"response\": \"${data.response}\", \"query\": \"${data.query}\"}},\n",
    "        \"groundedness\": {\n",
    "            \"column_mapping\": {\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"context\": \"${data.ground_truth}\",\n",
    "                \"ground_truth\": \"${data.ground_truth}\",\n",
    "                \"response\": \"${data.response}\"\n",
    "            } \n",
    "        },\n",
    "        \"relevance\": {\n",
    "            \"column_mapping\": {\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"context\": \"${data.ground_truth}\",\n",
    "                \"response\": \"${data.response}\"\n",
    "            } \n",
    "        },\n",
    "        \"fluency\": {\n",
    "            \"column_mapping\": {\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"context\": \"${data.ground_truth}\",\n",
    "                \"response\": \"${data.response}\"\n",
    "            } \n",
    "        },\n",
    "        \"similarity\": {\n",
    "            \"column_mapping\": {\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"context\": \"${data.ground_truth}\",\n",
    "                \"response\": \"${data.response}\"\n",
    "            } \n",
    "        },\n",
    "    },\n",
    "\n",
    "    # Specify the azure_ai_project to push results to portal\n",
    "    azure_ai_project = azure_ai_project,\n",
    "    \n",
    "    # Specify the output path to push results also to local file\n",
    "    output_path=\"./02-quality-evaluators.results.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fde8ca4",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "### 3.1 View Results Online\n",
    "\n",
    "Just as before, you can now view the results of the multi-evaluator run using the Evaluation tab in the Azure AI Foundry Studio. Here is what you should see:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903ce3da",
   "metadata": {},
   "source": [
    "#### Quality Evaluation\n",
    "\n",
    "![Quality](./../docs/img/screenshots/lab-02-portal-quality.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e146493",
   "metadata": {},
   "source": [
    "### 3.2 View Results Locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865e7bad",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Homework: Try It Yourself\n",
    "\n",
    "1. Import the necessary evaluator\n",
    "1. Invoke it with the relevant query/response parameters\n",
    "1. Print the results - **observe them**. Do you agree with assessment?\n",
    "1. Try changing the response - **re-evaluate** - Do you agree with the new assessment?\n",
    "1. Think of a scenario where you would use this evaluator - **write it down**.\n",
    "\n",
    "**Resources**:\n",
    "1. [Documentation](https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/evaluation-metrics-built-in?tabs=severity#generation-quality-metrics)\n",
    "1. [API Reference](https://learn.microsoft.com/en-us/python/api/azure-ai-evaluation/azure.ai.evaluation?view=azure-python-preview)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686630d4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  | Congratulations!\n",
    "\n",
    "You have successfully completed the second lab in this module and got hands-on experience with a core subset of the the built-in quality evaluators. You also got a sense of how to create and run a custom evaluator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dceed0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
