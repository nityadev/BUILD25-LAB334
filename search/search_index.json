{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Evaluate and improve the quality and safety of your AI applications\"","text":""},{"location":"#workshop-description","title":"Workshop Description","text":"<p>You want to build a custom AI application grounded in your enterprise data. How do you ensure response quality and safety so you can build the trust and confidence of your users?</p> <p>In this hands-on workshop, we'll explore the role of evaluations in the GenAIOps lifecycle and learn to use the Azure AI Foundry evaluations SDK to simulate test datasets, run built-in quality and safety evaluators, create custom evaluators, and visualize &amp; analyze evaluation results. </p> <p>Level: Intermediate/Advanced.  Duration: 75 minutes</p>"},{"location":"#learning-objectives","title":"Learning Objectives","text":"<p>This workshop helps you get a practical understanding of the capabilities of the Azure AI Foundry <code>azure-ai-evaluations</code> SDK. By completing the core Workshop track you'll be able to:</p> <ol> <li>Explain the role of evaluations at all stages of the GenAIOps lifecycle</li> <li>Use the SDK simulator to generate synthetic datasets for evaluations</li> <li>Use the SDK built-in evaluators for quality and safety assessments</li> <li>Use the SDK custom evaluator support to define and use custom metrics</li> <li>Use the SDK evaluate flow to run batch evaluations with multiple evaluators</li> <li>View and analyze evaluation results locally, and using the Azure AI Foundry portal</li> </ol>"},{"location":"#pre-requisites","title":"Pre-Requisites","text":"<p>IN-VENUE PARTICIPANTS: You will be given a pre-provisioned Azure Subscription for this lab</p> <p>AT-HOME PARTICIPANTS: You will provision your own Azure subscription using instructions provided</p> <p>To complete this lab you need:</p> <ul> <li> A personal GitHub account \u2192 create one for free if needed</li> <li> An Azure subscription \u2192 with quota for the required models</li> <li> Familiarity with Python \u2192 we'll make use of Jupyter notebooks</li> <li> Familiarity with Generative AI concepts and workflows</li> </ul>"},{"location":"#content-tracks","title":"Content Tracks","text":"<p>IN-VENUE PARTICIPANTS: We will prioritize the Workshop Track for completion!</p> <p>The content is structured in two parts:</p> <ol> <li>Workshop Track - learn core fundamentals in 75 minutes, in-venue</li> <li>Homework Track - continue learning at your own pace, at-home</li> </ol> <p>The entire lab is setup for self-paced learning at home, with your own Azure subscription. The guide provides instructions on how to do this under the \"Self-Guided\" tabs.</p>"},{"location":"#questions-feedback","title":"Questions &amp; Feedback","text":"<p>We welcome feedback to help us improve the learning experience. </p> <ol> <li>File an issue. We welcome feedback on ways to improve the workshop for future learners.</li> <li>Join the Azure AI Foundry Discord. Meet Azure AI community members and share insights.</li> <li>Visit the Azure AI Foundry Developer Forum. Get the latest updates on Azure AI Foundry.</li> </ol>"},{"location":"1-Workshop/01-Setup/00/","title":"1. Dev Environment","text":"<p>By the end of this section you should have</p> <ul> <li>An Azure subscription provisioned with required resources</li> <li>A GitHub Codespaces environment configured with env variables</li> <li> <p>Validated both with a set of simple checks</p> </li> <li> <p>DURATION: </p> <ul> <li>5 mins (In-Venue) - uses pre-provisioned resources</li> <li>20 mins (Self-Guided) - to also provision resources</li> </ul> </li> </ul>"},{"location":"1-Workshop/01-Setup/00/#before-you-begin","title":"Before You Begin","text":"<p>We designed this lab to be used at Microsoft Build (with a pre-provisioned Azure subscription) or at home (with your own Azure subscription). Just pick your path to get the right instructions.</p> <p>ARE YOU A MICROSOFT BUILD ATTENDEE OR A SELF-GUIDED LEARNER?</p> I AM A SESSION ATTENDEEI AM A SELF-GUIDED LEARNER <ul> <li> Yes, I am currently at Microsoft Build. </li> <li> I'll use a Skillable-provided Azure subscription**</li> </ul> <ul> <li>Yes, I am working on this at home. </li> <li>I'll be using my own Azure subscription**</li> </ul>"},{"location":"1-Workshop/01-Setup/00/#1-login-with-azure-cli","title":"1. Login with Azure CLI","text":"<p>You are currently running in a GitHub Codespaces environment. Let's authenticate to Azure so we can access resources from code.</p> <ol> <li> <p>Open the VS Code terminal and run this command - then follow instructions.</p> <pre><code>az login --use-device-code\n</code></pre> </li> <li> <p>You will be prompted to enter Azure credentials (username and password). Use the right credentials as described below:</p> I AM A SESSION ATTENDEEI AM A SELF-GUIDED LEARNER <ul> <li> Switch to the browser tab with the Skillable VM.</li> <li> Use the Azure credentials provided in the instructions panel there.</li> </ul> <ul> <li> Use your own Azure credentials to complete this login step.</li> </ul> <p>Return to the VS Code terminal - you should be logged in successfully. </p> </li> </ol>"},{"location":"1-Workshop/01-Setup/00/#2-setup-env-variables","title":"2. Setup Env Variables","text":"<p>Now, let's make sure our GitHub Codespaces environment can talk to these resources by setting the relevant environment variables. Return to the GitHub Codespaces tab now..</p> <ol> <li> <p>Run this command to create the <code>.env</code> file</p> <pre><code>cp .env.sample .env\n</code></pre> </li> <li> <p>Open the <code>.env</code> file created in the editor. It should look like this: </p> <pre><code># Azure Open AI\nAZURE_OPENAI_ENDPOINT=\nAZURE_OPENAI_API_KEY=\nAZURE_OPENAI_DEPLOYMENT=\"gpt-4o-mini\"\nAZURE_OPENAI_API_VERSION=\"2025-01-01-preview\"\n\n# Azure AI Foundry project\nAZURE_AI_CONNECTION_STRING=\n\n# Needed For Lab 0: Simulator\nAZURE_SEARCH_ENDPOINT=\nAZURE_SEARCH_API_KEY=\nAZURE_SEARCH_INDEX_NAME=\"contoso-products\"\n\n# Needed For Lab 1 on: Evaluator\nLAB_CHAT_MODEL=\"gpt-4o-mini\"\nLAB_JUDGE_MODEL=\"gpt-4\"\n</code></pre> <p>We'll fill these values in, next.</p> </li> </ol>"},{"location":"1-Workshop/01-Setup/00/#3-configure-azure-ai-project","title":"3. Configure Azure AI Project","text":"<p>Your Azure AI Foundry project is the main resource for developing and managing your generative AI application. Let's review it and configure our environment variables.</p> I AM A SESSION ATTENDEEI AM A SELF-GUIDED LEARNER <ul> <li>Visit https://ai.azure.com - login with the same Azure account.</li> <li>You'll see a pre-provisioned Azure AI project listed - click it to get the page below. </li> <li>Now use the highlighted sections to populate .env.<ul> <li> Set <code>AZURE_OPENAI_API_KEY</code> in .env - to API key value</li> <li> Set <code>AZURE_AI_CONNECTION_STRING</code> in .env - to Project connection string value</li> <li> Select the <code>Azure OpenAI</code> tab under \"Included capabilities\"</li> <li> Set <code>AZURE_OPENAI_ENDPOINT</code> in .env - to Azure OpenAI endpoint value</li> </ul> </li> </ul> <p></p> <ul> <li>Visit https://ai.azure.com - login with the same Azure account.</li> <li>You will see a Create project button - with no pre-existing projects.</li> <li>Create a new Azure AI project using a tutorial like this.</li> <li>Deploy 3 models (<code>gpt-4o-mini</code>, <code>gpt-4</code>, and <code>text-embedding-ada-002</code>) </li> <li>Add an Azure AI Search service resource (during setup, for convenience)</li> <li> <p>The created project overview looks like this. Leave this tab open - we'll use it later</p> <p> </p> </li> </ul>"},{"location":"1-Workshop/01-Setup/00/#4-configure-azure-ai-search","title":"4. Configure Azure AI Search","text":"<p>We use Azure AI Search indexes for simulating datasets, and exploring manual evaluation in the portal. To support this, we need to allow both types of API access. Let's do that, next.</p> <ol> <li>Open a new browser tab - visit https://portal.azure.com/#browse/resourcegroups</li> <li>Login with the same Azure account - click the <code>rg-AITOUR</code> entry to get the list of resources</li> <li>Click the Search service resource - click Settings then Keys in sidebar</li> <li> <p>In the API Access Control page below - click Both, then confirm \"Yes\" in pop-up</p> <p></p> </li> <li> <p>Now let's populate related .env variables</p> <ul> <li> Set <code>AZURE_SEARCH_API_KEY</code> in .env - to \"Primary Key\" from this page</li> <li> Switch to the Overview page for the Search service to see details</li> <li> Set <code>AZURE_SEARCH_ENDPOINT</code> in .env - to \"Url\" value (top right)</li> </ul> </li> </ol> <p>Your environment variables are set!</p>"},{"location":"1-Workshop/01-Setup/01/","title":"2. Lab Presentation","text":"<p>BY THE END OF THIS SECTION YOU SHOULD BE ABLE TO</p> <ul> <li>Explain the role of evaluations in the GenAIOps lifecycle</li> <li>Understand the key tools and capabilities of the Azure AI Evaluations SDK</li> <li>Understand how to apply these learnings to a relevant application scenario</li> </ul> I AM A SESSION ATTENDEEI AM A SELF-GUIDED LEARNER <p>Your instructor already covered this content - You can now GET STARTED ON LABS</p> <p>This section gives you some background for the labs. Review it to learn core concepts &amp; objectives.**</p>"},{"location":"1-Workshop/01-Setup/01/#1-the-genaiops-lifecycle","title":"1. The GenAIOps Lifecycle","text":"<p>Generative AI Operations (GenAIOps), refers to the practice of managing, evaluating, and improving generative AI systems to ensure they produce trustworthy, reliable, and safe outputs throughout their lifecycle. The GenAIOps cycle can be viewed as 3 stages:</p> <ol> <li>Model Selection - the first step is to find the right model for your needs</li> <li>App Development - the next step is to customize model behavior to suit requirements</li> <li>Operationalization - the last step is to monitor and optimize apps in production</li> </ol> <p></p> <p>Evaluations are critical to this process, helping us gain user confidence and trust in the quality and safety of our applications at each step:</p> <ol> <li>Model Selection - use a relevant dataset to evaluate models for fit</li> <li>App Development - use built-in and custom evaluators to assess quality and safety</li> <li>Operationalization - use tools to analyze results and optimize apps continuously</li> </ol>"},{"location":"1-Workshop/01-Setup/01/#2-the-application-scenario","title":"2. The Application Scenario","text":"<p>Understanding complex concepts is easier if we have an application scenario that we can use to contextualize the discussion. Let's revisit this popular application scenario. Contoso Outdoors is a fictional enterprise retailer that sells outdoor hiking and camping equipment on their website. The figure shows a mockup of that experience.</p> <p></p> <p>The popularity of the site has created a bottleneck for customer support. So they have asked you to build Contoso Chat - a RAG-based retail copilot that can answer questions grounded in the product catalog and customer purchase history.</p> <p></p> <p>NOTE: This workshop does NOT build Contoso Chat.</p> <p>We are using the application scenario to frame the discussion on evaluation in a real-world context. However, each lab will use a toy dataset or app to teach the tools, metrics, and processes for evaluation. If you are interested in learning to build Contoso Chat as an application, check out our previous AI Tour Workshops for details.</p> <p>You are a new hire in that team - and you are tasked with the following:</p> <ol> <li>Model Selection - find us the right model to use for the job</li> <li>Evaluation Dataset - get us the right dataset to use for evaluations</li> <li>Evaluation Metrics - identify evaluators we should use for quality &amp; safety</li> <li>Custom Evaluators - identify gaps in evaluation metrics that we should fill</li> </ol> <p>What do you do? Let's take you on the developer journey for evaluation.</p>"},{"location":"1-Workshop/01-Setup/01/#3-the-developer-journey","title":"3. The Developer Journey","text":"<p>The storyboard below visualizes the typical developer journey into evaluations. </p> <p></p> <p>Let's get a brief sense of what these steps involve:</p> <ol> <li>GenAIOps - end-to-end development of Generative AI apps starts with model selection.</li> <li>Model Selection - use evaluations to compare base models - and pick one for development.</li> <li>Dataset Generation - use simulators to create synthetic datasets - from different options.</li> <li>Evaluate Manually - try out the model with test prompts in the portal - before coding.</li> <li>Evaluate with SDK - use built-in and custom evaluators - with AI-assisted workflows.</li> <li>Customize It - build and run custom evaluators - address gaps in built-in metrics.</li> <li>Operationize It - use tools to analyze results and optimize apps continuously.</li> </ol> <p>In this lab, we will cover some of these elements in our main Workshop track - and provide bonus labs (\"Homework\") with learning resources to help you explore the rest at your own pace.</p>"},{"location":"1-Workshop/01-Setup/01/#references","title":"References","text":"<p>Three core resources to bookmark and revisit for this workshop:</p> <ul> <li>Azure AI Documentation</li> <li>Azure Best Practices for AI</li> <li>Azure AI Evaluation Tools</li> </ul>"},{"location":"1-Workshop/02-Labs/00/","title":"Lab 0: Simulate Datasets","text":"<p>In this lab, you will learn to generate a synthedic dataset using the Simulator.</p> <p>By the end of this lab, you should be should know:</p> <ul> <li> What the Simulator is and how to use it</li> <li> How to generate a synthetic dataset from a search index</li> <li> How to run an evaluation on the synthetic dataset</li> </ul>"},{"location":"1-Workshop/02-Labs/00/#motivation","title":"Motivation","text":"<p>When building a generative AI prototype, we can use a sample query to test and iterate on the prototype. But before we deploy this to production, we need to test it with a large and diverse set of queries to verify that its responses will always meet our desired quality and safety criteria.</p> <p>But where do we get this test dataset from? </p> <p>There are typically three options:</p> <ol> <li>Bring Your Own Data - e.g., customer support conversation records from a call center</li> <li>Create Test Dataset - e.g., manually create a small dataset to get started</li> <li>Create Synthetic Dataset - e.g., use tools to simulate relevant datasets for scale</li> </ol> <p>In this lab we explore option 3 - using the Simulator feature to generate synthetic data from a search index</p>"},{"location":"1-Workshop/02-Labs/00/#contoso-products","title":"Contoso Products","text":"<p>In this lab, we'll create an index called <code>contoso-products</code> using the data found in <code>data/product_info/products.csv</code>. Once created, we can then use it with the Simulator to generate a sample set of question-answer pairs based on our search index.</p> Example Product: CampBuddy Adventure Dining Table (click to expand) Attribute Value id 19 name Adventure Dining Table price 90.0 category Camping Tables brand CampBuddy description Discover the joy of outdoor adventures with the CampBuddy Adventure Dining Table. This feature-packed camping essential brings both comfort and convenience to your memorable trips. Made from high-quality aluminum, it promises long-lasting performance, weather resistance, and easy maintenance - all key for the great outdoors! It's light, portable, and comes with adjustable height settings to suit various seating arrangements and the spacious surface comfortably accommodates meals, drinks, and other essentials. The sturdy yet lightweight frame holds food, dishes, and utensils with ease. When it's time to pack up, it fold and stows away with no fuss, ready for the next adventure!  Perfect for camping, picnics, barbecues, and beach outings - its versatility shines as brightly as the summer sun! Durable, sturdy and a breeze to set up, the Adventure Dining Table will be a loyal companion on every trip. Embark on your next adventure and make lifetime memories with CampBuddy. As with all good experiences, it'll leave you wanting more!"},{"location":"1-Workshop/02-Labs/00/#1-populate-the-search-index","title":"1. Populate the Search Index","text":"<p>First, we need to create the search index and populate it with our data. For convenience, we have provided a dataset and a notebook that will do this for you. Just follow these two steps:</p>"},{"location":"1-Workshop/02-Labs/00/#11-update-access-roles","title":"1.1. Update Access Roles","text":"<p>We need our user identity to have specific access roles to the search resource - this step should take just a few seconds.</p> <ol> <li>Return to the GitHub Codespaces environment (browser tab)</li> <li> <p>Run this command in the VS Code terminal - it completes within a minute or so.</p> <pre><code>./scripts/update-search-roles.sh\n</code></pre> </li> </ol>"},{"location":"1-Workshop/02-Labs/00/#12-upload-the-data","title":"1.2 Upload The Data","text":"<p>We have a <code>data/product_info/products.csv</code> file with product data. In this step, we just run the notebook that uploads the data and creates a search index called <code>contoso-products</code>. This step should take just a minute or so to complete.</p> <ol> <li>Open the <code>data/product_info/create-azure-search.ipynb</code> notebook in editor.<ul> <li>Select Kernel - use the default Python environment (see: <code>3.12.10</code>)</li> <li>Click \"Clear Outputs\", then \"Run All\" - this takes 1-2 minutes to complete</li> <li>\u2705 | You should see: \"uploading 20 documents to index contoso-products\"</li> </ul> </li> </ol>"},{"location":"1-Workshop/02-Labs/00/#13-validate-index-creation","title":"1.3 Validate Index Creation","text":"<ol> <li> <p>Verify the product search index was created</p> <ul> <li>Return to Azure Portal browser tab - it should be open to the Search service page:     </li> <li>Click on Search explorer - you see <code>contoso-products</code> as shown. Click \"Search\" to see default search results for an empty query. It should look like this:     </li> </ul> </li> <li> <p>Test the Index</p> <ul> <li> <p>Copy this query into the search box and click Search:</p> <pre><code>Something to eat on\n</code></pre> </li> <li> <p>Look at the <code>value</code> field - you see something like this. You got a semantically meaningful result that goes beyond keyword matching. This is an example of vector search.</p> <p></p> </li> </ul> </li> </ol>"},{"location":"1-Workshop/02-Labs/00/#2-use-simulator-with-index","title":"2. Use Simulator with Index","text":"<p>All labs are setup as Jupyter notebooks - just follow these instructions:</p> <ol> <li>Open the <code>labs/00-simulate-datasets.ipynb</code> notebook in the VS Code editor.</li> <li>Click Select Kernel in the top right corner of the notebook - pick default Python kernel.</li> <li>Click Clear All Outputs in the top menu bar of the notebook - clears output from prior runs.</li> <li>Click Run All Cells in the top menu bar of the notebook - let the run complete.</li> </ol> <p>Now, review the notebook cell-by-cell to understand the steps. Answer questions or try alternative options when prompted, to build your understanding of the code.</p>"},{"location":"1-Workshop/02-Labs/01/","title":"Lab 1: Code-First Evaluation","text":"<p>In this lab, you will learn how to run your first evaluation using the SDK.</p> <p>In this lab, we look at the steps 3-5 of the GenAIOps lifecycle through the lens of the Azure AI Evaluations SDK. </p> <p></p> <p>By the end of this lab, you should be able to:</p> <ul> <li> Explain what the <code>evaluate</code> function does</li> <li> Know how to configure and run the <code>evaluate</code> function</li> <li> Run a single evaluator on a test dataset</li> <li> Save the evaluation results to a file</li> <li> View the evaluation results in the portal</li> </ul>"},{"location":"1-Workshop/02-Labs/01/#instructions","title":"Instructions","text":"<p>All labs are setup as Jupyter notebooks - just follow these instructions:</p> <ol> <li>Open the <code>labs/01-first-evaluate.ipynb</code> notebook in the VS Code editor.</li> <li>Click Select Kernel in the top right corner of the notebook - pick default Python kernel.</li> <li>Click Clear All Outputs in the top menu bar of the notebook - clears output from prior runs.</li> <li>Click Run All Cells in the top menu bar of the notebook - let the run complete.</li> </ol> <p>Now, review the notebook cell-by-cell to understand the steps. Answer questions or try alternative options when prompted, to build your understanding of the code.</p>"},{"location":"1-Workshop/02-Labs/02/","title":"Lab 2: Quality Evaluation","text":"<p>In this lab, you will learn about quality evaluations in Azure AI Foundry.</p> <p>By the end of this lab, you will know:</p> <ul> <li> What AI-Assisted evaluation workflows are, and how to run them.</li> <li> How to use built-in quality evaluators in Azure AI Foundry</li> <li> How to run a quality evaluator with a test prompt (to understand usage)</li> <li> How to run a composite quality evaluator (with multiple evaluators)</li> </ul>"},{"location":"1-Workshop/02-Labs/02/#ai-assisted-quality-evaluation","title":"AI-Assisted Quality Evaluation","text":"<p>The built-in evaluators in Azure AI Foundry use an AI-Assisted evaluation workflow.</p> <ol> <li>First, we identify a \"test dataset\" - with typical user prompts for the target app</li> <li>Next, we feed these to the application (chat model) - and collect responses</li> <li>Next, we feed responses (with inputs &amp; ground truth) to the evaluator (judge model)</li> <li>The evaluator grades the response and returns results (metrics) for various criteria.</li> </ol> <p></p>"},{"location":"1-Workshop/02-Labs/02/#built-in-quality-evaluators","title":"Built-in Quality Evaluators","text":"<p>The Azure AI Foundry platform provides a set of built-in quality evaluators as shown below. In this lab, we'll focus on exploring Generation and Custom evaluators - with a single example in each case.  You can then use the notebook as a sandbox to explore other evaluators on your own.</p> <p>Interested in Agents evaluation? Star this repo and revisit it for more lab notebooks post-Build!</p> <p></p>"},{"location":"1-Workshop/02-Labs/02/#instructions","title":"Instructions","text":"<p>All labs are setup as Jupyter notebooks - just follow these instructions:</p> <ol> <li>Open the <code>labs/02-quality-evaluators.ipynb</code> notebook in the VS Code editor.</li> <li>Click Select Kernel in the top right corner of the notebook - pick default Python kernel.</li> <li>Click Clear All Outputs in the top menu bar of the notebook - clears output from prior runs.</li> <li>Click Run All Cells in the top menu bar of the notebook - let the run complete.</li> </ol> <p>Now, review the notebook cell-by-cell to understand the steps. Answer questions or try alternative options when prompted, to build your understanding of the code.</p>"},{"location":"1-Workshop/02-Labs/03/","title":"Lab 3: Risk &amp; Safety Evaluation","text":"<p>In this lab, you will learn about risk &amp; safety evaluations in Azure AI Foundry.</p> <p>By the end of this lab, you will know:</p> <ul> <li> The built-in safety evaluators available in Azure AI Foundry</li> <li> How to run a safety evaluator with a test prompt (to understand usage)</li> <li> How to run a composite safety evaluator (to build a bette risk profile)</li> </ul>"},{"location":"1-Workshop/02-Labs/03/#ai-assisted-safety-evaluation","title":"AI-Assisted Safety Evaluation","text":"<p>Risk and safety evaluators draw on insights gained from our previous Large Language Model projects such as GitHub Copilot and Bing. This ensures a comprehensive approach to evaluating generated responses for risk and safety severity scores. </p> <p>These evaluators are generated through the Azure AI Foundry Evaluation service, which employs a set of LLMs. Each model is tasked with assessing specific risks that could be present in the response from your AI system (for example, sexual content, violent content, etc.). These evaluator models are provided with risk definitions and annotate accordingly. </p> <p></p>"},{"location":"1-Workshop/02-Labs/03/#built-in-safety-evaluators","title":"Built-in Safety Evaluators","text":"<p>Currently the following risks are supported: Hateful and unfair content \u00b7 Sexual content \u00b7 Violent content \u00b7 Self-harm-related content \u00b7 Protected material content \u00b7 Indirect attack jailbreak \u00b7 Direct attack jailbreak \u00b7 Code vulnerability \u00b7 Ungrounded attributes</p> <p>In this lab, we'll look at each of these with a test prompt, to gain intuition for how they assess the relevant risk or safety metrics. Then, we'll run the composite safety evaluator to get a sense for how we can build a more comprehensive risk profile for our AI system.</p> <p></p>"},{"location":"1-Workshop/02-Labs/03/#instructions","title":"Instructions","text":"<p>All labs are setup as Jupyter notebooks - just follow these instructions:</p> <ol> <li>Open the <code>labs/03-safety-evaluators.ipynb</code> notebook in the VS Code editor.</li> <li>Click Select Kernel in the top right corner of the notebook - pick default Python kernel.</li> <li>Click Clear All Outputs in the top menu bar of the notebook - clears output from prior runs.</li> <li>Click Run All Cells in the top menu bar of the notebook - let the run complete.</li> </ol> <p>Now, review the notebook cell-by-cell to understand the steps. Answer questions or try alternative options when prompted, to build your understanding of the code.</p>"},{"location":"1-Workshop/02-Labs/04/","title":"Lab 4: Evaluate Base Models","text":"<p>In this lab, you will how to evaluate base models for inital selection.</p> <p>By the end of this lab, you will know:</p> <ul> <li> How to compare multiple base models (during model selection)</li> <li> How to run evaluators across multiple models (for automated evaluation)</li> </ul>"},{"location":"1-Workshop/02-Labs/04/#deploy-another-model","title":"Deploy Another Model","text":"<p>Our current Azure AI Project has a <code>gpt-4o-mini</code> model deployed. Let's add a second deployment (<code>gpt-35-turbo</code>) and evaluate both with the same set of evaluators and dataset, to see how they compare.</p> <ol> <li>Return to the <code>ai.azure.com</code> tab</li> <li>Click on the default Azure AI Project, select Models + Endpoints</li> <li>Enter <code>gpt-35-turbo</code> in the search and select that model - click Confirm</li> <li>In the pop-up dialog, click Deploy - Done!</li> </ol> <p>This takes less than a minute - but gives you a sense of how easy it is to add more models to compare against if you want to assess fit for your application needs</p>"},{"location":"1-Workshop/02-Labs/04/#instructions","title":"Instructions","text":"<p>All labs are setup as Jupyter notebooks - just follow these instructions:</p> <ol> <li>Open the <code>labs/04-evaluate-models.ipynb</code> notebook in the VS Code editor.</li> <li>Click Select Kernel in the top right corner of the notebook - pick default Python kernel.</li> <li>Click Clear All Outputs in the top menu bar of the notebook - clears output from prior runs.</li> <li>Click Run All Cells in the top menu bar of the notebook - let the run complete.</li> </ol> <p>Now, review the notebook cell-by-cell to understand the steps. Answer questions or try alternative options when prompted, to build your understanding of the code.</p>"},{"location":"1-Workshop/03-Teardown/","title":"Summary &amp; Next Steps","text":""},{"location":"1-Workshop/03-Teardown/#1-tear-down","title":"1. Tear Down","text":"<p>TODO: Document teardown process for self-guided vs. skillable</p>"},{"location":"1-Workshop/03-Teardown/#2-give-feedback","title":"2. Give Feedback","text":"<p>TODO: Document survey links</p>"},{"location":"1-Workshop/03-Teardown/#3-next-steps","title":"3. Next Steps","text":"<p>We encourage you to keep going on your learning journey. Have questions or feedback? Here are some resources to help you:</p> <ol> <li>File an issue. We welcome feedback on ways to improve the workshop for future learners.</li> <li>Join the Azure AI Foundry Discord. Meet Azure AI community members and share insights.</li> <li>Visit the Azure AI Foundry Developer Forum. Get the latest updates on Azure AI Foundry.</li> </ol>"},{"location":"2-Homework/02/","title":"Lab 1: SDK Speed Run","text":"<p>Do this lab only if you are short on time. Otherwise, do the <code>02 Labs</code> section in order.</p> <ul> <li> It completes a tour of core evaluation SDK features in one notebook.</li> <li> Run the notebook at one shot - it takes about 5 minutes to complete execution.</li> <li> Use the remaining time to review the notebook cell-by-cell to understand the steps.</li> <li> Try out selective \"labs/\" notebooks after, to experiment with specific features.</li> </ul> <p>DURATION: 30 minutes</p>"},{"location":"2-Homework/02/#instructions","title":"Instructions","text":"<p>All labs are setup as Jupyter notebooks - just follow these instructions:</p> <ol> <li>Open <code>labs/00-speed-run/1-try-evaluation-sdk.ipynb</code> in the VS Code editor.</li> <li>Click Select Kernel in the top right corner of the notebook - pick default Python kernel.</li> <li>Click Clear All Outputs in the top menu bar of the notebook - clears output from prior runs.</li> <li>Click Run All Cells in the top menu bar of the notebook - let the run complete.</li> </ol> <p>Now, review the notebook cell-by-cell to understand the steps. Answer questions or try alternative options when prompted, to build your understanding of the code.</p>"},{"location":"2-Homework/02/#objectives","title":"Objectives","text":"<p>By the end of this lab, you should have:</p> <ul> <li> Verified that the evaluation SDK is installed and working</li> <li> Tried Generation quality evaluation - with an NLP evaluator</li> <li> Tried Generation quality evaluation - with built-in evaluator</li> <li> Tried Safety evaluation - - with built-in evaluator</li> <li> Tried Custom evaluation - with a code-based evaluator</li> <li> Run a Composite evaluation - with multiple evaluators</li> <li> Generated synthetic datasets - using the Simulator</li> <li> Run evaluation - and viewed results locally</li> <li> Run evaluation - and viewed results in portal</li> </ul>"},{"location":"2-Homework/06/","title":"Lab 6: Manual Evaluation","text":"<p>In this lab, you will learn how to do manual evaluations in the Azure AI portal.</p> <p>By the end of this lab, you will know:</p> <ul> <li> How to create a manual evaluation - with a test dataset (no code)</li> <li> How to create a manual evaluation - with your own data (grounded responses)</li> <li> How to rate the evaluation results interactively, and save for later review</li> </ul>"},{"location":"2-Homework/06/#instructions","title":"Instructions","text":"<p>All labs are setup as Jupyter notebooks - just follow these instructions:</p> <ol> <li>Open the <code>labs/06-manual-evaluation.ipynb</code> notebook in the VS Code editor.</li> <li>Click Select Kernel in the top right corner of the notebook - pick default Python kernel.</li> <li>Click Clear All Outputs in the top menu bar of the notebook - clears output from prior runs.</li> <li>Click Run All Cells in the top menu bar of the notebook - let the run complete.</li> </ol> <p>Now, review the notebook cell-by-cell to understand the steps. Answer questions or try alternative options when prompted, to build your understanding of the code.</p>"},{"location":"2-Homework/07/","title":"Lab 7: Evaluate Target","text":"<p>In this lab, you will learn how to do run evaluations against an application target.</p> <p>By the end of this lab, you will know:</p> <ul> <li> The different between evaluating a dataset and evaluating a target</li> <li> How to setup evaluations against a target (application endpoint)</li> <li> How to analyze the results of the evaluation</li> </ul>"},{"location":"2-Homework/07/#instructions","title":"Instructions","text":"<p>All labs are setup as Jupyter notebooks - just follow these instructions:</p> <ol> <li>Open the <code>labs/07-evaluate-target.ipynb</code> notebook in the VS Code editor.</li> <li>Click Select Kernel in the top right corner of the notebook - pick default Python kernel.</li> <li>Click Clear All Outputs in the top menu bar of the notebook - clears output from prior runs.</li> <li>Click Run All Cells in the top menu bar of the notebook - let the run complete.</li> </ol> <p>Now, review the notebook cell-by-cell to understand the steps. Answer questions or try alternative options when prompted, to build your understanding of the code.</p>"},{"location":"2-Homework/08/","title":"Lab 8: Agent Evaluation","text":"<p>In this lab, you will learn about new evaluation metrics for use with AI Agents</p> <p>By the end of this lab, you will know:</p> <ul> <li> How agent evaluation can differ from traditional app evaluation</li> <li> Agent-related built-in evaluators (and metrics) in Azure AI Foundry</li> <li> How to run the evaluations and analyze results for a sample agent</li> </ul>"},{"location":"2-Homework/08/#instructions","title":"Instructions","text":"<p>All labs are setup as Jupyter notebooks - just follow these instructions:</p> <ol> <li>Open the <code>labs/08-evaluate-agents.ipynb</code> notebook in the VS Code editor.</li> <li>Click Select Kernel in the top right corner of the notebook - pick default Python kernel.</li> <li>Click Clear All Outputs in the top menu bar of the notebook - clears output from prior runs.</li> <li>Click Run All Cells in the top menu bar of the notebook - let the run complete.</li> </ol> <p>Now, review the notebook cell-by-cell to understand the steps. Answer questions or try alternative options when prompted, to build your understanding of the code.</p>"}]}